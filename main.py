#main.py
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import roc_auc_score
import joblib
import numpy as np
from src.preprocessing import DataPreprocessor
from src.feature_engineering import FeatureEngineer
from src.models import MultiTaskTADFNet, XGBoostTADFPredictor
from src.training import ModelTrainer
from src.interpretation import ModelInterpreter
import pandas as pd
import os
from xgboost import XGBClassifier
os.makedirs('outputs', exist_ok=True)
os.makedirs('data/processed', exist_ok=True)
os.makedirs('data/splits', exist_ok=True)
def main():
    """ä¸»ç¨‹åº"""
    
    # 1. æ•°æ®é¢„å¤„ç†
    print("Step 1: Data Preprocessing...")
    preprocessor = DataPreprocessor('./data/all_conformers_data.csv')
    preprocessor.clean_data().extract_features_from_smiles().handle_missing_values().create_labels()
    # ========= æ£€æŸ¥å®˜èƒ½å›¢ç‰¹å¾æ˜¯å¦å­˜åœ¨ =========
    print("\n=== Checking functional group features in raw data ===")
    fg_features = ['has_nitro', 'count_nitro', 'has_triphenylamine', 
                'count_triphenylamine', 'has_carbazole', 'count_carbazole', 
                'has_triazine', 'count_triazine']

    for fg in fg_features:
        if fg in preprocessor.df.columns:
            non_zero = (preprocessor.df[fg] != 0).sum()
            print(f"{fg}: exists, non-zero count = {non_zero}")
        else:
            print(f"{fg}: NOT FOUND in data!")
    # ========= æ£€æŸ¥ç»“æŸ =========
    preprocessor.clean_data().handle_missing_values().create_labels()
    # é€‰æ‹©ä½¿ç”¨å“ªå¥—æ ‡ç­¾è¿›è¡Œè®­ç»ƒ
    USE_STRICT_LABELS = True  # å¯ä»¥é€šè¿‡å‚æ•°æ§åˆ¶

    if USE_STRICT_LABELS:
        print("\nğŸ“Š Using STRICT literature-based labels for training")
        # Primary labelså·²ç»åœ¨is_TADFå’Œis_rTADFä¸­
    else:
        print("\nğŸ“Š Using RELAXED labels for sensitivity analysis")
        # ä½¿ç”¨relaxedæ ‡ç­¾è¦†ç›–primary
        preprocessor.df['is_TADF'] = preprocessor.df['is_TADF_relaxed']
        preprocessor.df['is_rTADF'] = preprocessor.df['is_rTADF_relaxed']

    # æ‰“å°æœ€ç»ˆä½¿ç”¨çš„æ ‡ç­¾ç»Ÿè®¡
    print(f"Final labels for training:")
    print(f"  TADF: {preprocessor.df['is_TADF'].sum()}/{len(preprocessor.df)}")
    print(f"  rTADF: {preprocessor.df['is_rTADF'].sum()}/{len(preprocessor.df)}")
    # 1.5 åˆ†å­çº§èšåˆï¼ˆæ–°å¢ï¼‰
    print("\nStep 1.5: Molecular Level Aggregation...")

    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ„è±¡
    if 'Molecule' in preprocessor.df.columns:
        unique_molecules = preprocessor.df['Molecule'].nunique()
        total_conformers = len(preprocessor.df)
        avg_conformers = total_conformers / unique_molecules
        
        print(f"\n=== Molecular Statistics ===")
        print(f"Data structure: {unique_molecules} molecules, {total_conformers} conformers")
        print(f"Average conformers per molecule: {avg_conformers:.1f}")
        
        # è¯¦ç»†ç»Ÿè®¡
        mol_stats = preprocessor.df.groupby('Molecule').size()
        print(f"\nConformers distribution:")
        print(f"  Min: {mol_stats.min()}")
        print(f"  Max: {mol_stats.max()}")
        print(f"  Mean: {mol_stats.mean():.2f}")
        print(f"  Median: {mol_stats.median():.1f}")
        print(f"  Std: {mol_stats.std():.2f}")
        
        # æ£€æŸ¥æ ‡ç­¾ä¸€è‡´æ€§
        print(f"\nLabel consistency check:")
        for label in ['is_TADF', 'is_rTADF']:
            if label in preprocessor.df.columns:
                label_consistency = preprocessor.df.groupby('Molecule')[label].agg(['min', 'max', 'mean'])
                inconsistent = (label_consistency['min'] != label_consistency['max']).sum()
                if inconsistent > 0:
                    print(f"  âš ï¸ {inconsistent} molecules have inconsistent {label} labels across conformers")
                    # å¯é€‰ï¼šæ˜¾ç¤ºå“ªäº›åˆ†å­æœ‰ä¸ä¸€è‡´çš„æ ‡ç­¾
                    inconsistent_mols = label_consistency[label_consistency['min'] != label_consistency['max']].index[:5]
                    print(f"    Examples: {list(inconsistent_mols)}")
        
        # å†³å®šæ˜¯å¦éœ€è¦èšåˆ
        if avg_conformers > 1.5:  # æœ‰å¤šæ„è±¡
            print("\nâ†’ Multiple conformers detected, performing molecular aggregation...")
          #  preprocessor.aggregate_by_molecule(method='min_gap')
        else:
            print("\nâ†’ Single conformer per molecule, skipping aggregation")
    else:
        print("Warning: No 'Molecule' column found, cannot perform molecular aggregation")
    # 2. ç‰¹å¾å·¥ç¨‹
    print("\nStep 2: Feature Engineering...")
    fe = FeatureEngineer(preprocessor.df)
    # ========= æ£€æŸ¥Caliceneç‰¹å¾æ˜¯å¦å­˜åœ¨ =========
    print("\n=== Checking Calicene-specific features ===")

    # åŸºç¡€ç‰¹å¾
    basic_calicene = ['has_3ring', 'has_5ring', 'subs_on_3ring', 'subs_on_5ring']

    # ä½ç½®ç‰¹å¾
    position_features = ['num_in_subs_3ring', 'num_out_subs_3ring', 
                        'num_in_subs_5ring', 'num_out_subs_5ring',
                        'num_both_subs', 'num_sp_subs']

    # D/Aåˆ†å¸ƒ
    da_features = ['donor_on_3ring', 'donor_on_5ring',
                'acceptor_on_3ring', 'acceptor_on_5ring',
                'DA_strength_5minus3', 'DA_in_out_bias']

    # CTç‰¹å¾
    ct_features = ['CT_alignment_score', 'CT_position_weighted_score',
                'DA_asymmetry', 'favorable_for_inversion']

    # å¯†åº¦ç‰¹å¾
    density_features = ['D_density', 'A_density', 'D_volume_density', 
                    'A_volume_density', 'in_sub_density', 'out_sub_density']

    # One-hotç¼–ç ç‰¹å¾
    onehot_features = ['push_pull_pattern_none', 'push_pull_pattern_D5_A3',
                    'push_pull_pattern_D3_A5', 'push_pull_pattern_D5_only',
                    'push_pull_pattern_A3_only', 'push_pull_pattern_DD_balanced',
                    'push_pull_pattern_AA_balanced',
                    'ring_polarity_expected_aligned', 
                    'ring_polarity_expected_reversed',
                    'ring_polarity_expected_neutral']

    all_calicene_features = (basic_calicene + position_features + da_features + 
                            ct_features + density_features + onehot_features)

    found_count = 0
    missing_features = []

    for feat_group, feat_list in [
        ("Basic", basic_calicene),
        ("Position", position_features),
        ("D/A", da_features),
        ("CT", ct_features),
        ("Density", density_features),
        ("One-hot", onehot_features)
    ]:
        print(f"\n{feat_group} features:")
        for feat in feat_list:
            if feat in preprocessor.df.columns:
                non_zero = (preprocessor.df[feat] != 0).sum()
                print(f"  âœ“ {feat}: {non_zero} non-zero values")
                found_count += 1
            else:
                print(f"  âœ— {feat}: NOT FOUND")
                missing_features.append(feat)

    print(f"\nSummary: Found {found_count}/{len(all_calicene_features)} Calicene features")
    if missing_features:
        print(f"Missing features: {missing_features}")
    # ========= æ£€æŸ¥ç»“æŸ =========

    # åˆ›å»ºäº¤äº’ç‰¹å¾ï¼ˆåŒ…å«æ–¹æ³•1å’Œæ–¹æ³•2ï¼‰
    preprocessor.df = fe.create_interaction_features()
    print(f"Created interaction features. Shape: {preprocessor.df.shape}")
    # ========= æ–°å¢ï¼šåˆ†æé«˜å±‚æ¬¡ç‰¹å¾ =========
    # ç‰¹å¾ç¨€ç–æ€§åˆ†æ
    def analyze_feature_quality(df, features):
        """åˆ†æç‰¹å¾è´¨é‡"""
        quality_report = {}
        for feat in features:
            if feat in df.columns:
                non_zero = (df[feat] != 0).sum()
                unique = df[feat].nunique()
                quality_report[feat] = {
                    'coverage': non_zero / len(df),
                    'unique_values': unique
                }
        
        # æ‰“å°ä½è¦†ç›–ç‡ç‰¹å¾
        low_coverage = [f for f, v in quality_report.items() if v['coverage'] < 0.1]
        if low_coverage:
            print(f"Low coverage features (<10%): {low_coverage[:5]}")
        
        return quality_report
    
    # åˆ†æå…³é”®ç‰¹å¾è´¨é‡
    critical_features = fe.select_critical_features()
    feature_quality = analyze_feature_quality(preprocessor.df, critical_features)
    
    # ç§»é™¤ä½è´¨é‡ç‰¹å¾
    high_quality_features = [f for f in critical_features 
                            if f not in feature_quality or 
                            feature_quality[f]['coverage'] > 0.05]
    
    print(f"Filtered from {len(critical_features)} to {len(high_quality_features)} high-quality features")
    critical_features = high_quality_features
    # é€‰æ‹©å…³é”®ç‰¹å¾ï¼ˆæ–¹æ³•3ï¼‰
    # æ‰“å°å®˜èƒ½å›¢ç‰¹å¾çš„å­˜åœ¨æƒ…å†µ
    fg_features = ['donor_score', 'acceptor_score', 'D_A_ratio', 'D_A_product', 
                   'donor_homo_effect', 'acceptor_lumo_effect', 'DA_st_gap_effect']
    existing_fg = [f for f in fg_features if f in critical_features]
    print(f"Functional group features in critical features: {existing_fg}")
    # æ ‡å‡†åŒ–å…³é”®ç‰¹å¾
    # 1. äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆ0/1ï¼‰- ä¸æ ‡å‡†åŒ–
    # æ ‡å‡†åŒ–å…³é”®ç‰¹å¾
# 1. äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆ0/1ï¼‰- ä¸æ ‡å‡†åŒ–
    binary_features = [
        # CaliceneäºŒè¿›åˆ¶ç‰¹å¾
        'has_3ring', 'has_5ring',
        'donor_on_3ring', 'donor_on_5ring',
        'acceptor_on_3ring', 'acceptor_on_5ring',
        'favorable_for_inversion',
        
        # æ‰€æœ‰has_å¼€å¤´çš„ç‰¹å¾
        *[f for f in critical_features if f.startswith('has_')],
        
        # One-hotç¼–ç ç‰¹å¾
        *[f for f in critical_features if 'push_pull_pattern_' in f],
        *[f for f in critical_features if 'ring_polarity_expected_' in f],
        
        # å…¶ä»–äºŒè¿›åˆ¶
        'is_D_A_molecule', 'has_inversion'
    ]

    # 2. æ•´æ•°è®¡æ•°ç‰¹å¾ - ä¸æ ‡å‡†åŒ–æˆ–ä½¿ç”¨ç‰¹æ®Šå¤„ç†
    count_features = [
        # ç¯ç³»ç»Ÿè®¡æ•°
        'num_rings', 'num_aromatic_rings', 'num_saturated_rings',
        'num_aliphatic_rings', 'num_aromatic_heterocycles',
        'num_saturated_heterocycles',
        'num_3_member_rings', 'num_4_member_rings', 'num_5_member_rings',
        'num_6_member_rings', 'num_7_member_rings', 'num_8_member_rings',
        
        # åŸå­è®¡æ•°
        'num_atoms', 'num_bonds', 'num_heavy_atoms',
        'num_heteroatoms', 'num_N_atoms', 'num_O_atoms', 'num_S_atoms',
        'num_F_atoms', 'num_Cl_atoms', 'num_Br_atoms', 'num_I_atoms', 'num_P_atoms',
        
        # å…¶ä»–è®¡æ•°
        'num_rotatable_bonds', 'num_conjugated_bonds', 'num_conjugated_systems',
        'num_hbd', 'num_hba', 'num_amide_bonds',
        
        # Caliceneè®¡æ•°
        'subs_on_3ring', 'subs_on_5ring',
        'num_in_subs_3ring', 'num_out_subs_3ring',
        'num_in_subs_5ring', 'num_out_subs_5ring',
        'num_both_subs', 'num_sp_subs',
        
        # æ‰€æœ‰count_å¼€å¤´çš„ç‰¹å¾
        *[f for f in critical_features if f.startswith('count_')],
        
        # åè½¬gapè®¡æ•°
        'num_inverted_gaps', 'num_primary_inversions',
        
        # CRESTæ„è±¡æ•°
        'crest_num_conformers',
        # æ¿€å‘æ€è®¡æ•°
        'num_singlet_states', 'num_triplet_states',
        
        # æˆåŠŸæ ‡å¿—ï¼ˆè™½ç„¶æ˜¯0/1ï¼Œä½†ä½œä¸ºè®¡æ•°å¤„ç†ï¼‰
        'excited_opt_success', 'excited_no_imaginary',
        'triplet_excited_opt_success', 'triplet_excited_no_imaginary'
    ]

    # 3. åˆ†ç±»/ç¼–ç ç‰¹å¾ - ä¸æ ‡å‡†åŒ–
    categorical_features = [
        'mol_type_code',  # åˆ†å­ç±»å‹ç¼–ç ï¼ˆ0,1,2,3ï¼‰
    ]

    # 4. éœ€è¦æ ‡å‡†åŒ–çš„è¿ç»­ç‰¹å¾
    continuous_features = []
    features_to_skip = set(binary_features + count_features + categorical_features)

    # å¼ºåˆ¶åŒ…å«å¯†åº¦ç‰¹å¾ï¼ˆå³ä½¿å®ƒä»¬å¯èƒ½ä¸åœ¨critical_featuresä¸­ï¼‰
    must_normalize = [
            'cyano_density', 'nitro_density', 'amino_density', 'carbonyl_density',
            'aromatic_gap_product', 'st_gap_ratio', 'st_average_energy'
        ]

    for feat in must_normalize:
        if feat in preprocessor.df.columns and feat not in features_to_skip:
            continuous_features.append(feat)
            print(f"  Added mandatory continuous feature: {feat}")
    
    # ç„¶åæ·»åŠ å…¶ä»–è¿ç»­ç‰¹å¾
    for f in critical_features:
        if f in preprocessor.df.columns and f not in features_to_skip and f not in continuous_features:
            unique_vals = preprocessor.df[f].dropna().unique()
            if len(unique_vals) <= 10:  # å°‘äº10ä¸ªå”¯ä¸€å€¼
                print(f"  Feature {f} has only {len(unique_vals)} unique values, checking if categorical...")
                # å¦‚æœéƒ½æ˜¯æ•´æ•°ï¼Œå¯èƒ½æ˜¯ç¼–ç ç‰¹å¾
                if all(isinstance(v, (int, np.integer)) or v.is_integer() for v in unique_vals if pd.notna(v)):
                    print(f"    -> Treating as categorical, not normalizing")
                    categorical_features.append(f)
                    continue
            continuous_features.append(f)

    print(f"\nFeature type breakdown:")
    print(f"  Binary features (keep as-is): {len([f for f in binary_features if f in preprocessor.df.columns])}")
    print(f"  Count features (keep as-is): {len([f for f in count_features if f in preprocessor.df.columns])}")
    print(f"  Categorical features (keep as-is): {len([f for f in categorical_features if f in preprocessor.df.columns])}")
    print(f"  Continuous features (normalize): {len(continuous_features)}")
    preprocessor.binary_features = binary_features
    preprocessor.count_features = count_features
    preprocessor.categorical_features = categorical_features
    preprocessor.continuous_features = continuous_features
    # åªæ ‡å‡†åŒ–è¿ç»­ç‰¹å¾
    if continuous_features:
        print(f"\nNormalizing {len(continuous_features)} continuous features...")
        # æ˜¾ç¤ºä¸€äº›å°†è¢«æ ‡å‡†åŒ–çš„ç‰¹å¾ç¤ºä¾‹
        print(f"  Examples: {continuous_features[:5]}")
        preprocessor.normalize_features(continuous_features)

    # ä¿å­˜ç‰¹å¾ç±»å‹ä¿¡æ¯ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨
    preprocessor.continuous_features = continuous_features  # æ·»åŠ è¿™è¡Œ

    # éªŒè¯å…³é”®ç‰¹å¾æ²¡æœ‰è¢«ç ´å
    print("\n=== Verification of Key Features ===")
    verification_features = [
        'has_3ring', 'has_5ring', 'num_rings', 'num_aromatic_rings',
        'num_3_member_rings', 'num_atoms', 'subs_on_3ring', 'subs_on_5ring'
    ]

    for feat in verification_features:
        if feat in preprocessor.df.columns:
            unique_vals = preprocessor.df[feat].nunique()
            non_zero = (preprocessor.df[feat] != 0).sum()
            mean_val = preprocessor.df[feat].mean()
            max_val = preprocessor.df[feat].max()
            print(f"{feat:25s}: unique={unique_vals:4d}, non_zero={non_zero:4d}, mean={mean_val:7.3f}, max={max_val:7.3f}")

    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    import os
    os.makedirs('data/processed', exist_ok=True)

    # ä¿å­˜å®Œæ•´æ•°æ®å‰ï¼Œç§»é™¤å…¨é›¶åˆ—
    print("\n=== Checking for empty columns in full data ===")
    full_df = preprocessor.df.copy()
    empty_cols_full = []
    for col in full_df.columns:
        if col not in ['Molecule', 'is_TADF', 'is_rTADF']:  # ä¿æŠ¤æ ‡ç­¾åˆ—
            if full_df[col].dtype in [np.number, float, int]:
                if (full_df[col] == 0).all() or full_df[col].isna().all():
                    empty_cols_full.append(col)
                    print(f"  Removing empty column from full data: {col}")

    if empty_cols_full:
        full_df = full_df.drop(columns=empty_cols_full)
        print(f"Removed {len(empty_cols_full)} empty columns from full data")

    full_df.to_csv('data/processed/full_features.csv', index=False)
    print(f"Saved full_features.csv: {full_df.shape}")

    # ä¿å­˜å…³é”®ç‰¹å¾å­é›† - ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—
    save_columns = critical_features + ['is_TADF', 'is_rTADF', 'Molecule', 'SMILES']
    save_columns = [col for col in save_columns if col in preprocessor.df.columns]
    critical_df = preprocessor.df[save_columns].copy()
    # éªŒè¯å…³é”®ç‰¹å¾
    print("\n=== Feature Verification ===")
    verify_features = ['has_3ring_nh2', 'has_5ring_nh2', 'has_5ring_oh', 
                    'num_aromatic_rings', 'has_nitro', 'nitro_density']
    for feat in verify_features:
        if feat in critical_df.columns:
            non_zero = (critical_df[feat] != 0).sum()
            print(f"{feat}: {non_zero} non-zero values")
    # ç§»é™¤å…¨é›¶åˆ—å’Œç©ºåˆ—
    print("\n=== Checking for empty columns in critical features ===")
    empty_cols_critical = []
    for col in save_columns:
        if col not in ['is_TADF', 'is_rTADF', 'Molecule']:  # ä¿æŠ¤æ ‡ç­¾å’ŒIDåˆ—
            if col in critical_df.columns:
                # æ£€æŸ¥æ˜¯å¦å…¨é›¶æˆ–å…¨ç©º
                if critical_df[col].dtype in [np.number, float, int]:
                    is_all_zero = (critical_df[col] == 0).all()
                    is_all_nan = critical_df[col].isna().all()
                    if is_all_zero or is_all_nan:
                        empty_cols_critical.append(col)
                        print(f"  Removing empty column: {col} (all_zero={is_all_zero}, all_nan={is_all_nan})")

    # ç§»é™¤ç©ºåˆ—
    if empty_cols_critical:
        critical_df = critical_df.drop(columns=empty_cols_critical)
        save_columns = [col for col in save_columns if col not in empty_cols_critical]
        # åŒæ—¶ä»critical_featuresä¸­ç§»é™¤ï¼Œä¿æŒä¸€è‡´æ€§
        critical_features = [f for f in critical_features if f not in empty_cols_critical]
        print(f"Removed {len(empty_cols_critical)} empty columns from critical features")

    # å†æ¬¡éªŒè¯ä¿å­˜çš„æ•°æ®
    print("\n=== Final verification before saving ===")
    verification_features = [
        'has_3ring', 'has_5ring', 'num_rings', 'num_aromatic_rings',
        'num_3_member_rings', 'num_atoms', 'subs_on_3ring', 'subs_on_5ring'
    ]

    for feat in verification_features:
        if feat in critical_df.columns:
            non_zero = (critical_df[feat] != 0).sum()
            unique_vals = critical_df[feat].nunique()
            print(f"  {feat}: {non_zero} non-zero values, {unique_vals} unique values")

    critical_df.to_csv('data/processed/critical_features.csv', index=False)
    print(f"Saved critical_features.csv: {critical_df.shape}")

    # æ‰“å°ä¸¤ä¸ªæ–‡ä»¶çš„å·®å¼‚ç»Ÿè®¡
    print(f"\n=== Summary ===")
    print(f"Full features: {full_df.shape[1]} columns (removed {len(empty_cols_full)} empty)")
    print(f"Critical features: {critical_df.shape[1]} columns (removed {len(empty_cols_critical)} empty)")
    print(f"Feature reduction: {full_df.shape[1]} -> {critical_df.shape[1]} ({critical_df.shape[1]/full_df.shape[1]*100:.1f}%)")
    
    # 3. æ•°æ®åˆ’åˆ†
    print("Step 3: Data Splitting...")
    (X_train, X_val, X_test), (y_tadf_train, y_tadf_val, y_tadf_test), \
    (y_rtadf_train, y_rtadf_val, y_rtadf_test) = preprocessor.split_data( 
        use_features=critical_features 
    )

    # éªŒè¯ç»´åº¦
    print(f"\nFeature dimensions check:")
    print(f"  Training features: {X_train.shape}")
    print(f"  Critical features selected: {len(critical_features)}")
    print(f"  Continuous features normalized: {len(continuous_features)}")  # ä¿®æ”¹è¿™é‡Œï¼šnumeric_features -> continuous_features

    if hasattr(preprocessor, 'feature_cols'):
        print(f"  Number of feature names: {len(preprocessor.feature_cols)}")
        assert len(preprocessor.feature_cols) == X_train.shape[1], \
            f"Feature names mismatch: {len(preprocessor.feature_cols)} names vs {X_train.shape[1]} features"
    # ============ éªŒè¯ä»£ç ç»“æŸ ============
    # è½¬æ¢ä¸ºTensor
    X_train_tensor = torch.FloatTensor(X_train)
    y_tadf_train_tensor = torch.FloatTensor(y_tadf_train)
    y_rtadf_train_tensor = torch.FloatTensor(y_rtadf_train)
    
    # åˆ›å»ºDataLoader
    train_dataset = TensorDataset(X_train_tensor, y_tadf_train_tensor, y_rtadf_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    # 4. æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ
    print("Step 4: Training Deep Learning Model...")
    # è®¡ç®—ç±»åˆ«æƒé‡
    n_tadf_pos = y_tadf_train.sum()
    n_tadf_neg = len(y_tadf_train) - n_tadf_pos
    tadf_pos_weight = n_tadf_neg / (n_tadf_pos + 1e-6)

    n_rtadf_pos = y_rtadf_train.sum()
    n_rtadf_neg = len(y_rtadf_train) - n_rtadf_pos
    rtadf_pos_weight = n_rtadf_neg / (n_rtadf_pos + 1e-6)

    print(f"Class weights - TADF: {tadf_pos_weight:.2f}, rTADF: {rtadf_pos_weight:.2f}")

    dl_model = MultiTaskTADFNet(input_dim=X_train.shape[1])
    trainer = ModelTrainer(dl_model, tadf_pos_weight=tadf_pos_weight, rtadf_pos_weight=rtadf_pos_weight)

    # è®­ç»ƒå¾ªç¯ - æ”¹ä¸ºç›‘æ§PR-AUC
    best_val_pr_auc = 0  # æ”¹ä¸ºç›‘æ§PR-AUC
    patience = 20
    patience_counter = 0

    for epoch in range(100):
        train_loss = trainer.train_epoch(train_loader)
        
        # éªŒè¯
        val_dataset = TensorDataset(
            torch.FloatTensor(X_val),
            torch.FloatTensor(y_tadf_val),
            torch.FloatTensor(y_rtadf_val)
        )
        val_loader = DataLoader(val_dataset, batch_size=32)
        val_metrics = trainer.evaluate(val_loader, use_optimal_threshold=True)
        
        # ä½¿ç”¨PR-AUCçš„å¹³å‡å€¼ä½œä¸ºç›‘æ§æŒ‡æ ‡
        val_pr_auc = (val_metrics.get('tadf_pr_auc', 0) + val_metrics.get('rtadf_pr_auc', 0)) / 2
        
        print(f"Epoch {epoch+1}: Train Loss={train_loss:.4f}, "
            f"TADF PR-AUC={val_metrics.get('tadf_pr_auc', 0):.4f}, "
            f"rTADF PR-AUC={val_metrics.get('rtadf_pr_auc', 0):.4f}, "
            f"TADF F1={val_metrics.get('tadf_f1', 0):.4f}, "
            f"rTADF F1={val_metrics.get('rtadf_f1', 0):.4f}")
        
        # æ—©åœåŸºäºPR-AUC
        if val_pr_auc > best_val_pr_auc:
            best_val_pr_auc = val_pr_auc
            torch.save(dl_model.state_dict(), 'best_model.pth')
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("Early stopping triggered")
                break
        
        trainer.scheduler.step(val_pr_auc)  # åŸºäºPR-AUCè°ƒæ•´å­¦ä¹ ç‡
        
    # 5. XGBoostæ¨¡å‹è®­ç»ƒ
    print("\nStep 5: Training XGBoost Model...")
    xgb_predictor = XGBoostTADFPredictor()
    xgb_predictor.train(X_train, y_tadf_train, y_rtadf_train)
    
    # 6. æ¨¡å‹è¯„ä¼°
    print("\nStep 6: Model Evaluation...")
    
    # æ·±åº¦å­¦ä¹ æ¨¡å‹è¯„ä¼°
    dl_model.load_state_dict(torch.load('best_model.pth'))
    test_dataset = TensorDataset(
        torch.FloatTensor(X_test),
        torch.FloatTensor(y_tadf_test),
        torch.FloatTensor(y_rtadf_test)
    )
    test_loader = DataLoader(test_dataset, batch_size=32)
    dl_metrics = trainer.evaluate(test_loader)
    
    print("\nDeep Learning Model Performance:")
    for key, value in dl_metrics.items():
        print(f"{key}: {value:.4f}")
    
    # XGBoostæ¨¡å‹è¯„ä¼°
    xgb_tadf_pred, xgb_rtadf_pred = xgb_predictor.predict(X_test)
    xgb_metrics = {
        'tadf_auc': roc_auc_score(y_tadf_test, xgb_tadf_pred) if len(np.unique(y_tadf_test)) > 1 else 0,
        'rtadf_auc': roc_auc_score(y_rtadf_test, xgb_rtadf_pred) if len(np.unique(y_rtadf_test)) > 1 else 0
    }
    
    print("\nXGBoost Model Performance:")
    for key, value in xgb_metrics.items():
        print(f"{key}: {value:.4f}")
    
    # ============ 7. æ¨¡å‹è§£é‡Š ============
    print("\nStep 7: Model Interpretation...")
    
    # è·å–ç‰¹å¾åç§°
    if hasattr(preprocessor, 'feature_cols'):
        feature_names = preprocessor.feature_cols
    else:
        feature_names = critical_features  # ä½¿ç”¨å…³é”®ç‰¹å¾åç§°
    
    # ç¡®ä¿ç‰¹å¾åç§°æ•°é‡æ­£ç¡®
    if len(feature_names) != X_train.shape[1]:
        print(f"Warning: Feature names mismatch. Using generic names.")
        feature_names = [f'Feature_{i}' for i in range(X_train.shape[1])]
    
    # åˆ›å»ºè§£é‡Šå™¨
    interpreter = ModelInterpreter(
        xgb_predictor.tadf_model.best_estimator_ if hasattr(xgb_predictor.tadf_model, 'best_estimator_') 
        else xgb_predictor.tadf_model,
        X_train,
        feature_names
    )
    
    # SHAPåˆ†æ
    interpreter.shap_analysis(X_test[:min(100, len(X_test))], task='tadf')
    
    # ç‰¹å¾é‡è¦æ€§
    tadf_importance, rtadf_importance = xgb_predictor.get_feature_importance()

    # ç»˜åˆ¶TADFç‰¹å¾é‡è¦æ€§
    if isinstance(tadf_importance, dict):
        tadf_values = list(tadf_importance.values())
    else:
        tadf_values = tadf_importance

    interpreter.plot_feature_importance(tadf_values, top_n=20, task='tadf')

    # ç»˜åˆ¶rTADFç‰¹å¾é‡è¦æ€§
    if isinstance(rtadf_importance, dict):
        rtadf_values = list(rtadf_importance.values())
    else:
        rtadf_values = rtadf_importance

    interpreter.plot_feature_importance(rtadf_values, top_n=20, task='rtadf')

    # å¯é€‰ï¼šåˆ›å»ºç»¼åˆç‰¹å¾é‡è¦æ€§å›¾ï¼ˆä¸¤ä¸ªä»»åŠ¡çš„å¹³å‡ï¼‰
    combined_importance = (np.array(tadf_values) + np.array(rtadf_values)) / 2
    interpreter.plot_feature_importance(combined_importance, top_n=20, task='combined')

    print("Generated feature importance plots for both tasks")
    
    
    # æ··æ·†çŸ©é˜µ
    # æ··æ·†çŸ©é˜µ - TADF
    xgb_tadf_binary = (xgb_tadf_pred > 0.5).astype(int)
    interpreter.plot_confusion_matrices(y_tadf_test, xgb_tadf_binary, 
                                    labels=['Non-TADF', 'TADF'], 
                                    task='tadf')

    # æ··æ·†çŸ©é˜µ - rTADF
    xgb_rtadf_binary = (xgb_rtadf_pred > 0.5).astype(int)
    interpreter.plot_confusion_matrices(y_rtadf_test, xgb_rtadf_binary, 
                                    labels=['Non-rTADF', 'rTADF'], 
                                    task='rtadf')
    # ä¿å­˜æ¨¡å‹
    import joblib
    joblib.dump(xgb_predictor, 'xgboost_tadf_predictor.pkl')
    torch.save({
        'model_state_dict': dl_model.state_dict(),
        'model_config': {'input_dim': X_train.shape[1], 'hidden_dims': [256, 128, 64], 'dropout': 0.3}
    }, 'best_dl_model.pth')
    print("Models saved successfully!")

    print("\nStep 8: Saving Models and Data...")
    
    # ä¿å­˜æ¨¡å‹
    import joblib
    
    # ä¿å­˜XGBoostæ¨¡å‹
    joblib.dump(xgb_predictor, 'xgboost_tadf_predictor.pkl')
    
    # ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹
    torch.save({
        'model_state_dict': dl_model.state_dict(),
        'model_config': {'input_dim': X_train.shape[1], 'hidden_dims': [256, 128, 64], 'dropout': 0.3}
    }, 'best_dl_model.pth')
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    if hasattr(preprocessor, 'scaler'):
        joblib.dump(preprocessor.scaler, 'scaler.pkl')
        print("Scaler saved successfully!")
    
    # ä¿å­˜ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆç”¨äºé¢„æµ‹æ—¶ï¼‰
    with open('data/splits/feature_names.txt', 'w') as f:
        for feat in feature_names:
            f.write(f"{feat}\n")
    print(f"Saved {len(feature_names)} feature names")
    
    print("All models and data saved successfully!")

    # ============ 9. ç¨³å¥æ€§æµ‹è¯• ============
    print("\n" + "="*60)
    print("Step 9: Robustness Testing")
    print("="*60)
    
    from src.robustness_test import (
        define_feature_blocks, run_block_ablation,
        xgb_importances, perm_importance, shap_importance, rank_consistency,
        sweep_thresholds, oof_score, multi_seed_stability,
        get_design_safe_features, safe_cv_xgb_grouped, random_label_test
    )
    
    # 9.0 å‡†å¤‡ä¸¤å¥—ç‰¹å¾ï¼šè¯Šæ–­æ¨¡å‹ vs è®¾è®¡æ¨¡å‹
    print("\n9.0 Feature Sets Preparation...")
    
    # è¯Šæ–­æ¨¡å‹ç‰¹å¾ï¼ˆåŒ…å«gapï¼‰
    diagnostic_features = preprocessor.feature_cols
    print(f"Diagnostic model features: {len(diagnostic_features)}")
    
    # è®¾è®¡æ¨¡å‹ç‰¹å¾ï¼ˆç§»é™¤è¿‘æ ‡ç­¾ç‰¹å¾ï¼‰
    design_features = get_design_safe_features(preprocessor.df)
    design_features = [f for f in design_features if f in preprocessor.feature_cols]
    print(f"Design model features: {len(design_features)}")
    
    # 9.1 éšæœºæ ‡ç­¾æµ‹è¯•ï¼ˆæ³„æ¼æ£€æµ‹ï¼‰
    print("\n9.1 Random Label Test (Leakage Detection)...")
    
    # æµ‹è¯•è¯Šæ–­æ¨¡å‹
    print("\nDiagnostic Model Random Label Test:")
    diag_random = random_label_test(preprocessor.df, diagnostic_features, n_iterations=3)
    
    # æµ‹è¯•è®¾è®¡æ¨¡å‹
    print("\nDesign Model Random Label Test:")
    design_random = random_label_test(preprocessor.df, design_features, n_iterations=3)
    
    # 9.2 åˆ†å—æ¶ˆèï¼ˆä½¿ç”¨GroupKFoldï¼‰
    print("\n9.2 Block Ablation Analysis (with GroupKFold)...")
    blocks = define_feature_blocks(preprocessor.df.columns)
    
    # æ›´æ–°run_block_ablationä»¥ä½¿ç”¨safe_cv_xgb_grouped
    def run_block_ablation_grouped(df, target_col, all_feats, blocks):
        """æ”¹è¿›çš„åˆ†å—æ¶ˆèï¼Œä½¿ç”¨GroupKFold"""
        results = []
        
        # å®šä¹‰å®éªŒ
        only_gap = [f for f in blocks['gap_block'] if f in all_feats]
        only_da_geom = sorted(set([f for f in blocks['da_block'] + blocks['geom_block'] if f in all_feats]))
        only_structure = [f for f in blocks['structure_block'] if f in all_feats]
        
        full = all_feats
        drop_gap = [c for c in full if c not in set(blocks['gap_block'])]
        drop_da = [c for c in full if c not in set(blocks['da_block'])]
        drop_geom = [c for c in full if c not in set(blocks['geom_block'])]
        
        exps = {
            'only_gap': only_gap,
            'only_da_geom': only_da_geom,
            'only_structure': only_structure,
            'full': full,
            'full_minus_gap': drop_gap,
            'full_minus_da': drop_da,
            'full_minus_geom': drop_geom
        }
        
        for name, cols in exps.items():
            if len(cols) == 0:
                continue
            
            print(f"  Testing {name} with {len(cols)} features...")
            m = safe_cv_xgb_grouped(df, cols, target_col, n_splits=5)
            m['setting'] = name
            m['n_feats'] = len(cols)
            results.append(m)
        
        return pd.DataFrame(results).sort_values('roc_auc', ascending=False)
    
    # è®¾è®¡æ¨¡å‹çš„æ¶ˆèï¼ˆä¸åŒ…å«gapï¼‰
    print("\nDesign Model Ablation (no gap features):")
    design_blocks = define_feature_blocks(pd.Index(design_features))
    res_design_rtadf = run_block_ablation_grouped(
        preprocessor.df, 'is_rTADF', design_features, design_blocks
    )
    print(res_design_rtadf[['setting', 'n_feats', 'roc_auc', 'f1', 'roc_auc_std']].to_string())
    res_design_rtadf.to_csv('outputs/design_rtadf_ablation.csv', index=False)
    
    # è¯Šæ–­æ¨¡å‹çš„æ¶ˆèï¼ˆåŒ…å«gapï¼‰
    print("\nDiagnostic Model Ablation (with gap features):")
    res_diag_rtadf = run_block_ablation_grouped(
        preprocessor.df, 'is_rTADF', diagnostic_features, blocks
    )
    print(res_diag_rtadf[['setting', 'n_feats', 'roc_auc', 'f1', 'roc_auc_std']].to_string())
    res_diag_rtadf.to_csv('outputs/diagnostic_rtadf_ablation.csv', index=False)
    
    # 9.3 æ”¹è¿›çš„é‡è¦æ€§ä¸€è‡´æ€§ï¼ˆåŒä¸€éªŒè¯é›†ï¼‰
    print("\n9.3 Improved Feature Importance Consistency...")
    
    # ä½¿ç”¨è®¾è®¡æ¨¡å‹ç‰¹å¾è¿›è¡Œæµ‹è¯•
    from sklearn.model_selection import GroupShuffleSplit
    
    X_design = preprocessor.df[design_features].astype(np.float32).fillna(0).values
    y_rtadf = preprocessor.df['is_rTADF'].astype(int).values
    groups = preprocessor.df['Molecule'].values if 'Molecule' in preprocessor.df.columns else None
    
    # åˆ†ç»„åˆ’åˆ†
    if groups is not None:
        gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
        tr_idx, val_idx = next(gss.split(X_design, y_rtadf, groups))
    else:
        from sklearn.model_selection import train_test_split
        tr_idx, val_idx = train_test_split(
            range(len(X_design)), test_size=0.2, random_state=42, stratify=y_rtadf
        )
    
    X_tr, X_val = X_design[tr_idx], X_design[val_idx]
    y_tr, y_val = y_rtadf[tr_idx], y_rtadf[val_idx]
    
    # è®­ç»ƒæ¨¡å‹
    from sklearn.impute import SimpleImputer
    from sklearn.pipeline import Pipeline
    
    pipe = Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('model', XGBClassifier(
            n_estimators=400, max_depth=3, learning_rate=0.05,
            subsample=0.9, colsample_bytree=0.9, random_state=42
        ))
    ])
    pipe.fit(X_tr, y_tr)
    
    # åœ¨åŒä¸€éªŒè¯é›†ä¸Šè®¡ç®—ä¸‰ç§é‡è¦æ€§
    model = pipe.named_steps['model']
    imputer = pipe.named_steps['imputer']
    X_val_imp = imputer.transform(X_val)
    
    # XGBoostå†…ç½®é‡è¦æ€§
    imp_xgb = xgb_importances(model)
    
    # ç½®æ¢é‡è¦æ€§ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
    imp_perm = perm_importance(
        pipe, X_val, y_val, design_features,
        scoring='average_precision' if y_val.mean() < 0.1 else 'roc_auc',
        n_repeats=5  # å‡å°‘é‡å¤æ¬¡æ•°åŠ å¿«é€Ÿåº¦
    )
    
    # SHAPé‡è¦æ€§ï¼ˆåœ¨éªŒè¯é›†ä¸Šï¼‰
    imp_shap = shap_importance(
        model, X_val_imp, design_features, n_sample=min(300, len(X_val))
    )
    
    # è®¡ç®—ä¸€è‡´æ€§
    if len(imp_xgb) > 0 and len(imp_perm) > 0 and len(imp_shap) > 0:
        rho_gp, j_gp = rank_consistency(imp_xgb, 'gain', imp_perm, 'perm_importance', topk=20)
        rho_gs, j_gs = rank_consistency(imp_xgb, 'gain', imp_shap, 'shap_mean_abs', topk=20)
        rho_ps, j_ps = rank_consistency(imp_perm, 'perm_importance', imp_shap, 'shap_mean_abs', topk=20)
        
        print(f"\nDesign Model Importance Consistency:")
        print(f"  Spearman(gain vs perm): {rho_gp:.3f}, Jaccard@20: {j_gp:.2f}")
        print(f"  Spearman(gain vs SHAP): {rho_gs:.3f}, Jaccard@20: {j_gs:.2f}")
        print(f"  Spearman(perm vs SHAP): {rho_ps:.3f}, Jaccard@20: {j_ps:.2f}")
        
        if rho_ps < 0.5 or j_ps < 0.4:
            print("  âš ï¸ WARNING: Low consistency between importance measures")
    
   # 9.4 æ”¹è¿›çš„å¤–æ¨ç¨³å®šæ€§ï¼ˆä½¿ç”¨åˆ†ç»„ï¼‰
    print("\n9.4 Enhanced Extrapolation Stability (with Groups)...")
    
    # å‡†å¤‡åˆ†ç»„ä¿¡æ¯
    if 'Molecule' in preprocessor.df.columns:
        groups = preprocessor.df['Molecule'].values
        print(f"Using molecular groups: {len(np.unique(groups))} unique molecules")
    else:
        groups = np.arange(len(preprocessor.df))
        print("Warning: No Molecule column, using pseudo-groups")
    
    # ä½¿ç”¨è®¾è®¡æ¨¡å‹ç‰¹å¾çš„åˆ†ç»„OOFè¯„åˆ†
    from src.robustness_test import oof_score_grouped
    
    X_design = preprocessor.df[design_features].astype(np.float32).fillna(0).values
    y_rtadf = preprocessor.df['is_rTADF'].astype(int).values
    
    oof_metrics, oof_probs = oof_score_grouped(
        X_design, y_rtadf, groups,
        seeds=(7, 13, 23), 
        n_splits=5
    )
    
    print(f"\nDesign Model Grouped OOF Results:")
    for metric, value in oof_metrics.items():
        if not np.isnan(value):
            print(f"  {metric}: {value:.4f}")
    
    # éªŒè¯ï¼šOOFåº”è¯¥ä¸æ¶ˆèç»“æœæ¥è¿‘
    if abs(oof_metrics['roc_auc'] - 0.625) > 0.15:  # é¢„æœŸåœ¨0.625é™„è¿‘
        print("  âš ï¸ WARNING: OOF AUC significantly different from ablation results!")
        print("  This may indicate remaining data leakage.")
    else:
        print("  âœ“ OOF results consistent with ablation analysis")
    
    # è¯Šæ–­æ¨¡å‹çš„åˆ†ç»„OOFï¼ˆä½œä¸ºå¯¹æ¯”ï¼‰
    X_diag = preprocessor.df[diagnostic_features].astype(np.float32).fillna(0).values
    
    oof_diag_metrics, _ = oof_score_grouped(
        X_diag, y_rtadf, groups,
        seeds=(7, 13, 23),
        n_splits=5
    )
    
    print(f"\nDiagnostic Model Grouped OOF Results:")
    for metric, value in oof_diag_metrics.items():
        if not np.isnan(value):
            print(f"  {metric}: {value:.4f}") 
    # 9.5 æ¦‚ç‡æ ¡å‡†å’ŒBootstrapç½®ä¿¡åŒºé—´
    print("\n9.5 Probability Calibration and Bootstrap CI...")

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦èƒ½è¿›è¡Œæ ¡å‡†åˆ†æ
    if len(design_features) == 0:
        print("  Skipping calibration - no design features available")
    else:
        try:
            from src.robustness_test import (
                calibrate_probabilities, plot_calibration_curve, 
                bootstrap_confidence_intervals
            )
            
            # é‡æ–°å‡†å¤‡è®¾è®¡æ¨¡å‹çš„æ•°æ®ï¼ˆä½¿ç”¨design_featuresè€Œä¸æ˜¯critical_featuresï¼‰
            # è·å–åŸå§‹çš„åˆ†ç»„ä¿¡æ¯
            if 'Molecule' in preprocessor.df.columns:
                from sklearn.model_selection import GroupShuffleSplit
                
                # å‡†å¤‡è®¾è®¡ç‰¹å¾æ•°æ®
                X_design_full = preprocessor.df[design_features].fillna(0).values.astype(np.float32)
                y_full = preprocessor.df['is_rTADF'].values
                groups_full = preprocessor.df['Molecule'].values
                
                # ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­é‡ç°split
                gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
                train_val_idx, test_idx = next(gss.split(X_design_full, y_full, groups_full))
                
                # å†åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†
                X_trainval = X_design_full[train_val_idx]
                y_trainval = y_full[train_val_idx]
                groups_trainval = groups_full[train_val_idx]
                
                val_ratio = 0.1 / 0.8  # ç›¸å½“äºæ€»ä½“çš„10%
                gss2 = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=42)
                train_idx, val_idx = next(gss2.split(X_trainval, y_trainval, groups_trainval))
                
                # æœ€ç»ˆçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨design_featuresï¼‰
                X_train_design = X_trainval[train_idx]
                X_test_design = X_design_full[test_idx]
                y_train_design = y_trainval[train_idx]
                y_test_design = y_full[test_idx]
                
                print(f"  Design model data shapes:")
                print(f"    Train: {X_train_design.shape}")
                print(f"    Test: {X_test_design.shape}")
                
                # è®­ç»ƒæœ€ç»ˆæ¨¡å‹ç”¨äºæ ¡å‡†
                final_model = XGBClassifier(
                    n_estimators=300, max_depth=3, learning_rate=0.05,
                    subsample=0.9, colsample_bytree=0.9, random_state=42
                )
                final_model.fit(X_train_design, y_train_design)
                
                # æ¦‚ç‡æ ¡å‡†
                prob_uncal, prob_cal, calibrated_model = calibrate_probabilities(
                    final_model, 
                    X_train_design, 
                    y_train_design,
                    X_test_design,
                    method='isotonic'
                )
                
                # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
                ece_before, ece_after = plot_calibration_curve(
                    y_test_design, prob_uncal, prob_cal, task='design_rtadf'
                )
                print(f"  ECE before calibration: {ece_before:.4f}")
                print(f"  ECE after calibration: {ece_after:.4f}")
                
                # Bootstrapç½®ä¿¡åŒºé—´
                print("\nBootstrap Confidence Intervals (1000 iterations)...")
                ci_results = bootstrap_confidence_intervals(
                    X_test_design, y_test_design, final_model, n_bootstrap=1000
                )
                
                print(f"  ROC-AUC: {ci_results['roc_auc_mean']:.3f} "
                    f"(95% CI: [{ci_results['roc_auc_ci'][0]:.3f}, {ci_results['roc_auc_ci'][1]:.3f}])")
                print(f"  PR-AUC: {ci_results['pr_auc_mean']:.3f} "
                    f"(95% CI: [{ci_results['pr_auc_ci'][0]:.3f}, {ci_results['pr_auc_ci'][1]:.3f}])")
                
                # ä¿å­˜æœ€ç»ˆæŠ¥å‘Šï¼ˆåŒ…å«æ ¡å‡†ç»“æœï¼‰
                final_summary = pd.DataFrame({
                    'Model': ['Diagnostic', 'Design', 'Design_Calibrated'],
                    'ROC-AUC': [oof_diag_metrics['roc_auc'], 
                                oof_metrics['roc_auc'],
                                ci_results['roc_auc_mean']],
                    'PR-AUC': [oof_diag_metrics.get('pr_auc', 0), 
                            oof_metrics.get('pr_auc', 0),
                            ci_results['pr_auc_mean']],
                    'ECE': [np.nan, ece_before, ece_after],
                    'F1': [oof_diag_metrics.get('f1', 0),
                        oof_metrics.get('f1', 0),
                        np.nan],
                    'Optimal_Threshold': [oof_diag_metrics.get('optimal_threshold', 0.5),
                                        oof_metrics.get('optimal_threshold', 0.5),
                                        np.nan]
                })
                
            else:
                print("  Warning: No molecular groups found, skipping calibration")
                raise ValueError("No molecular groups")
                
        except Exception as e:
            print(f"  Calibration analysis failed: {e}")
            print("  Saving results without calibration...")
            
            # ä¿å­˜åŸºæœ¬ç»“æœï¼ˆä¸åŒ…å«æ ¡å‡†ï¼‰
            final_summary = pd.DataFrame({
                'Model': ['Diagnostic', 'Design'],
                'ROC-AUC': [oof_diag_metrics['roc_auc'], 
                            oof_metrics['roc_auc']],
                'PR-AUC': [oof_diag_metrics.get('pr_auc', 0), 
                        oof_metrics.get('pr_auc', 0)],
                'F1': [oof_diag_metrics.get('f1', 0),
                    oof_metrics.get('f1', 0)],
                'Optimal_Threshold': [oof_diag_metrics.get('optimal_threshold', 0.5),
                                    oof_metrics.get('optimal_threshold', 0.5)]
            })
        
        final_summary.to_csv('outputs/final_model_summary.csv', index=False)
        print("\nFinal summary saved to outputs/final_model_summary.csv")

    print("\n" + "="*60)
    print("Analysis complete!")
    print("="*60)

if __name__ == "__main__":
    main()